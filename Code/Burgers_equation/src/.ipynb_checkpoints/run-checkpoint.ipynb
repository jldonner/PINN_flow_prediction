{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9a30c3-4abd-490d-92e6-45c9727b37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ Adam optimization ~~\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Loss: 4263.444920259658 time: 3.909 iter: 0/500\n",
      "Loss: 4195.915605230846 time: 0.519 iter: 1/500\n",
      "Loss: 4232.618972759356 time: 0.427 iter: 2/500\n",
      "Loss: 4224.8776848533025 time: 0.428 iter: 3/500\n",
      "Loss: 4199.735149766206 time: 0.435 iter: 4/500\n",
      "Loss: 4190.7108574677 time: 0.459 iter: 5/500\n",
      "Loss: 4198.640503967 time: 0.425 iter: 6/500\n",
      "Loss: 4207.158568570231 time: 0.464 iter: 7/500\n",
      "Loss: 4206.008210113945 time: 0.450 iter: 8/500\n",
      "Loss: 4198.161118323705 time: 0.439 iter: 9/500\n",
      "Loss: 4191.346054450342 time: 0.459 iter: 10/500\n",
      "Loss: 4190.1584234637485 time: 0.466 iter: 11/500\n",
      "Loss: 4193.716160703565 time: 0.434 iter: 12/500\n",
      "Loss: 4197.595730762412 time: 0.429 iter: 13/500\n",
      "Loss: 4198.1429363955285 time: 0.421 iter: 14/500\n",
      "Loss: 4195.274052041339 time: 0.467 iter: 15/500\n",
      "Loss: 4191.548813883746 time: 0.435 iter: 16/500\n",
      "Loss: 4189.5464605921725 time: 0.460 iter: 17/500\n",
      "Loss: 4190.013444944086 time: 0.433 iter: 18/500\n",
      "Loss: 4191.782267332583 time: 0.458 iter: 19/500\n",
      "Loss: 4193.095094158064 time: 0.436 iter: 20/500\n",
      "Loss: 4192.967313182059 time: 0.447 iter: 21/500\n",
      "Loss: 4191.607954202544 time: 0.462 iter: 22/500\n",
      "Loss: 4189.998059896645 time: 0.420 iter: 23/500\n",
      "Loss: 4189.1346975158995 time: 0.539 iter: 24/500\n",
      "Loss: 4189.376287620821 time: 0.430 iter: 25/500\n",
      "Loss: 4190.262209977335 time: 0.437 iter: 26/500\n",
      "Loss: 4190.9517033676 time: 0.445 iter: 27/500\n",
      "Loss: 4190.917310969303 time: 0.435 iter: 28/500\n",
      "Loss: 4190.271026282653 time: 0.441 iter: 29/500\n",
      "Loss: 4189.523129461308 time: 0.428 iter: 30/500\n",
      "Loss: 4189.129641382284 time: 0.479 iter: 31/500\n",
      "Loss: 4189.222615589375 time: 0.532 iter: 32/500\n",
      "Loss: 4189.606112865231 time: 0.557 iter: 33/500\n",
      "Loss: 4189.923612665347 time: 0.540 iter: 34/500\n",
      "Loss: 4189.907222928454 time: 0.525 iter: 35/500\n",
      "Loss: 4189.570410129976 time: 0.555 iter: 36/500\n",
      "Loss: 4189.17375697209 time: 0.548 iter: 37/500\n",
      "Loss: 4188.982937712136 time: 0.564 iter: 38/500\n",
      "Loss: 4189.059516499469 time: 0.535 iter: 39/500\n",
      "Loss: 4189.264427039228 time: 0.550 iter: 40/500\n",
      "Loss: 4189.407803189618 time: 0.502 iter: 41/500\n",
      "Loss: 4189.382113038257 time: 0.547 iter: 42/500\n",
      "Loss: 4189.212545913436 time: 0.539 iter: 43/500\n",
      "Loss: 4189.027351106895 time: 0.534 iter: 44/500\n",
      "Loss: 4188.95608647596 time: 0.555 iter: 45/500\n",
      "Loss: 4189.021696511691 time: 0.524 iter: 46/500\n",
      "Loss: 4189.132667088175 time: 0.434 iter: 47/500\n",
      "Loss: 4189.180858894296 time: 0.500 iter: 48/500\n",
      "Loss: 4189.1316595651515 time: 0.499 iter: 49/500\n",
      "Loss: 4189.029317541156 time: 0.530 iter: 50/500\n",
      "Loss: 4188.947355478675 time: 0.516 iter: 51/500\n",
      "Loss: 4188.935048721938 time: 0.508 iter: 52/500\n",
      "Loss: 4188.983404458761 time: 0.470 iter: 53/500\n",
      "Loss: 4189.034646831049 time: 0.483 iter: 54/500\n",
      "Loss: 4189.037367166943 time: 0.500 iter: 55/500\n",
      "Loss: 4188.992334650632 time: 0.489 iter: 56/500\n",
      "Loss: 4188.940609043197 time: 0.480 iter: 57/500\n",
      "Loss: 4188.919280316262 time: 0.539 iter: 58/500\n",
      "Loss: 4188.934943648618 time: 0.460 iter: 59/500\n",
      "Loss: 4188.96549506066 time: 0.501 iter: 60/500\n",
      "Loss: 4188.979419060499 time: 0.489 iter: 61/500\n",
      "Loss: 4188.962799786289 time: 0.480 iter: 62/500\n",
      "Loss: 4188.93150471671 time: 0.500 iter: 63/500\n",
      "Loss: 4188.912333416966 time: 0.529 iter: 64/500\n",
      "Loss: 4188.915705049631 time: 0.489 iter: 65/500\n",
      "Loss: 4188.9308862456 time: 0.500 iter: 66/500\n",
      "Loss: 4188.940455798585 time: 0.490 iter: 67/500\n",
      "Loss: 4188.934718828564 time: 0.503 iter: 68/500\n",
      "Loss: 4188.918422475743 time: 0.480 iter: 69/500\n",
      "Loss: 4188.906024493789 time: 0.539 iter: 70/500\n",
      "Loss: 4188.906757686922 time: 0.471 iter: 71/500\n",
      "Loss: 4188.915768376146 time: 0.468 iter: 72/500\n",
      "Loss: 4188.9217837146025 time: 0.559 iter: 73/500\n",
      "Loss: 4188.919026123319 time: 0.485 iter: 74/500\n",
      "Loss: 4188.910370306149 time: 0.445 iter: 75/500\n",
      "Loss: 4188.903318031145 time: 0.589 iter: 76/500\n",
      "Loss: 4188.903210263024 time: 0.480 iter: 77/500\n",
      "Loss: 4188.908013310388 time: 0.460 iter: 78/500\n",
      "Loss: 4188.911018867094 time: 0.475 iter: 79/500\n",
      "Loss: 4188.908742068384 time: 0.482 iter: 80/500\n",
      "Loss: 4188.9036363124405 time: 0.491 iter: 81/500\n",
      "Loss: 4188.900164265166 time: 0.535 iter: 82/500\n",
      "Loss: 4188.90059104483 time: 0.494 iter: 83/500\n",
      "Loss: 4188.903381627501 time: 0.477 iter: 84/500\n",
      "Loss: 4188.9048191121165 time: 0.459 iter: 85/500\n",
      "Loss: 4188.903126027832 time: 0.463 iter: 86/500\n",
      "Loss: 4188.900197365959 time: 0.476 iter: 87/500\n",
      "Loss: 4188.8987587774445 time: 0.564 iter: 88/500\n",
      "Loss: 4188.899456052511 time: 0.489 iter: 89/500\n",
      "Loss: 4188.900865724416 time: 0.509 iter: 90/500\n",
      "Loss: 4188.901104148851 time: 0.487 iter: 91/500\n",
      "Loss: 4188.899687031831 time: 0.499 iter: 92/500\n",
      "Loss: 4188.898029811566 time: 0.470 iter: 93/500\n",
      "Loss: 4188.897612452243 time: 0.539 iter: 94/500\n",
      "Loss: 4188.898301170119 time: 0.449 iter: 95/500\n",
      "Loss: 4188.898928812293 time: 0.470 iter: 96/500\n",
      "Loss: 4188.898674086684 time: 0.444 iter: 97/500\n",
      "Loss: 4188.8977214954575 time: 0.435 iter: 98/500\n",
      "Loss: 4188.896999931235 time: 0.489 iter: 99/500\n",
      "Loss: 4188.897092237006 time: 0.479 iter: 100/500\n",
      "Loss: 4188.897563042918 time: 0.451 iter: 101/500\n",
      "Loss: 4188.897665949872 time: 0.413 iter: 102/500\n",
      "Loss: 4188.8972281686 time: 0.479 iter: 103/500\n",
      "Loss: 4188.896653088087 time: 0.494 iter: 104/500\n",
      "Loss: 4188.896431660591 time: 0.500 iter: 105/500\n",
      "Loss: 4188.896628029112 time: 0.442 iter: 106/500\n",
      "Loss: 4188.8968195501175 time: 0.477 iter: 107/500\n",
      "Loss: 4188.896674154952 time: 0.450 iter: 108/500\n",
      "Loss: 4188.896326663055 time: 0.520 iter: 109/500\n",
      "Loss: 4188.89609852059 time: 0.463 iter: 110/500\n",
      "Loss: 4188.896127680181 time: 0.486 iter: 111/500\n",
      "Loss: 4188.896265366405 time: 0.469 iter: 112/500\n",
      "Loss: 4188.896254505165 time: 0.443 iter: 113/500\n",
      "Loss: 4188.896053737538 time: 0.486 iter: 114/500\n",
      "Loss: 4188.895860836927 time: 0.490 iter: 115/500\n",
      "Loss: 4188.895819198797 time: 0.480 iter: 116/500\n",
      "Loss: 4188.895878506319 time: 0.435 iter: 117/500\n",
      "Loss: 4188.895895713096 time: 0.474 iter: 118/500\n",
      "Loss: 4188.895795527297 time: 0.460 iter: 119/500\n",
      "Loss: 4188.89565799368 time: 0.479 iter: 120/500\n",
      "Loss: 4188.895602832421 time: 0.549 iter: 121/500\n",
      "Loss: 4188.895627823843 time: 0.512 iter: 122/500\n",
      "Loss: 4188.895643111163 time: 0.490 iter: 123/500\n",
      "Loss: 4188.895592337721 time: 0.479 iter: 124/500\n",
      "Loss: 4188.89550249712 time: 0.492 iter: 125/500\n",
      "Loss: 4188.895446569765 time: 0.477 iter: 126/500\n",
      "Loss: 4188.895447169415 time: 0.459 iter: 127/500\n",
      "Loss: 4188.895452264396 time: 0.430 iter: 128/500\n",
      "Loss: 4188.895418482665 time: 0.500 iter: 129/500\n",
      "Loss: 4188.895358684361 time: 0.470 iter: 130/500\n",
      "Loss: 4188.89531351588 time: 0.469 iter: 131/500\n",
      "Loss: 4188.895303187855 time: 0.490 iter: 132/500\n",
      "Loss: 4188.895302612033 time: 0.479 iter: 133/500\n",
      "Loss: 4188.895279020877 time: 0.514 iter: 134/500\n",
      "Loss: 4188.8952370286015 time: 0.465 iter: 135/500\n",
      "Loss: 4188.895202448936 time: 0.479 iter: 136/500\n",
      "Loss: 4188.8951874784225 time: 0.490 iter: 137/500\n",
      "Loss: 4188.895179958176 time: 0.459 iter: 138/500\n",
      "Loss: 4188.895159740765 time: 0.478 iter: 139/500\n",
      "Loss: 4188.895126939316 time: 0.476 iter: 140/500\n",
      "Loss: 4188.895098728099 time: 0.487 iter: 141/500\n",
      "Loss: 4188.895082836729 time: 0.489 iter: 142/500\n",
      "Loss: 4188.895071380985 time: 0.460 iter: 143/500\n",
      "Loss: 4188.895052999505 time: 0.470 iter: 144/500\n",
      "Loss: 4188.895026877656 time: 0.475 iter: 145/500\n",
      "Loss: 4188.895003323447 time: 0.464 iter: 146/500\n",
      "Loss: 4188.894987539517 time: 0.489 iter: 147/500\n",
      "Loss: 4188.894973792437 time: 0.480 iter: 148/500\n",
      "Loss: 4188.894955531401 time: 0.453 iter: 149/500\n",
      "Loss: 4188.894932937055 time: 0.480 iter: 150/500\n",
      "Loss: 4188.894912009037 time: 0.444 iter: 151/500\n",
      "Loss: 4188.894895975967 time: 0.476 iter: 152/500\n",
      "Loss: 4188.8948809343865 time: 0.469 iter: 153/500\n",
      "Loss: 4188.894862863839 time: 0.483 iter: 154/500\n",
      "Loss: 4188.894842642967 time: 0.486 iter: 155/500\n",
      "Loss: 4188.894823775812 time: 0.480 iter: 156/500\n",
      "Loss: 4188.894807765739 time: 0.459 iter: 157/500\n",
      "Loss: 4188.894792042027 time: 0.469 iter: 158/500\n",
      "Loss: 4188.894774196782 time: 0.490 iter: 159/500\n",
      "Loss: 4188.894755318825 time: 0.472 iter: 160/500\n",
      "Loss: 4188.894737581569 time: 0.457 iter: 161/500\n",
      "Loss: 4188.894721422327 time: 0.480 iter: 162/500\n",
      "Loss: 4188.894705194358 time: 0.480 iter: 163/500\n",
      "Loss: 4188.8946876360515 time: 0.487 iter: 164/500\n",
      "Loss: 4188.894669726802 time: 0.471 iter: 165/500\n",
      "Loss: 4188.894652771208 time: 0.474 iter: 166/500\n",
      "Loss: 4188.894636626892 time: 0.485 iter: 167/500\n",
      "Loss: 4188.894620221447 time: 0.450 iter: 168/500\n",
      "Loss: 4188.894603027208 time: 0.479 iter: 169/500\n",
      "Loss: 4188.894585810072 time: 0.489 iter: 170/500\n",
      "Loss: 4188.894569288344 time: 0.490 iter: 171/500\n",
      "Loss: 4188.894553119199 time: 0.470 iter: 172/500\n",
      "Loss: 4188.894536654925 time: 0.469 iter: 173/500\n",
      "Loss: 4188.894519803408 time: 0.479 iter: 174/500\n",
      "Loss: 4188.894503109587 time: 0.470 iter: 175/500\n",
      "Loss: 4188.894486891389 time: 0.469 iter: 176/500\n",
      "Loss: 4188.894470785508 time: 0.460 iter: 177/500\n",
      "Loss: 4188.894454439792 time: 0.465 iter: 178/500\n",
      "Loss: 4188.894437957134 time: 0.467 iter: 179/500\n",
      "Loss: 4188.89442167612 time: 0.471 iter: 180/500\n",
      "Loss: 4188.894405673346 time: 0.508 iter: 181/500\n",
      "Loss: 4188.894389661902 time: 0.478 iter: 182/500\n",
      "Loss: 4188.894373500609 time: 0.506 iter: 183/500\n",
      "Loss: 4188.89435735191 time: 0.476 iter: 184/500\n",
      "Loss: 4188.894341392339 time: 0.477 iter: 185/500\n",
      "Loss: 4188.894325580319 time: 0.469 iter: 186/500\n",
      "Loss: 4188.894309729683 time: 0.483 iter: 187/500\n",
      "Loss: 4188.894293821924 time: 0.476 iter: 188/500\n",
      "Loss: 4188.894277996109 time: 0.464 iter: 189/500\n",
      "Loss: 4188.894262311813 time: 0.500 iter: 190/500\n",
      "Loss: 4188.89424669189 time: 0.499 iter: 191/500\n",
      "Loss: 4188.894231043372 time: 0.484 iter: 192/500\n",
      "Loss: 4188.8942154057695 time: 0.485 iter: 193/500\n",
      "Loss: 4188.894199868486 time: 0.480 iter: 194/500\n",
      "Loss: 4188.894184426202 time: 0.470 iter: 195/500\n",
      "Loss: 4188.894169014083 time: 0.479 iter: 196/500\n",
      "Loss: 4188.894153604917 time: 0.469 iter: 197/500\n",
      "Loss: 4188.8941382481535 time: 0.473 iter: 198/500\n",
      "Loss: 4188.894122982342 time: 0.476 iter: 199/500\n",
      "Loss: 4188.894107776358 time: 0.479 iter: 200/500\n",
      "Loss: 4188.894092593864 time: 0.490 iter: 201/500\n",
      "Loss: 4188.894091075924 time: 0.449 iter: 202/500\n",
      "Loss: 4188.894089556472 time: 0.552 iter: 203/500\n",
      "Loss: 4188.894088036188 time: 0.467 iter: 204/500\n",
      "Loss: 4188.894086515792 time: 0.469 iter: 205/500\n",
      "Loss: 4188.894084995847 time: 0.460 iter: 206/500\n",
      "Loss: 4188.894083476621 time: 0.480 iter: 207/500\n",
      "Loss: 4188.894081958062 time: 0.460 iter: 208/500\n",
      "Loss: 4188.894080439842 time: 0.457 iter: 209/500\n",
      "Loss: 4188.894078921447 time: 0.472 iter: 210/500\n",
      "Loss: 4188.894077402286 time: 0.473 iter: 211/500\n",
      "Loss: 4188.894075881777 time: 0.456 iter: 212/500\n",
      "Loss: 4188.894074359411 time: 0.480 iter: 213/500\n",
      "Loss: 4188.894072834796 time: 0.460 iter: 214/500\n",
      "Loss: 4188.89407130767 time: 0.479 iter: 215/500\n",
      "Loss: 4188.894069777898 time: 0.480 iter: 216/500\n",
      "Loss: 4188.894068245473 time: 0.459 iter: 217/500\n",
      "Loss: 4188.894066710479 time: 0.469 iter: 218/500\n",
      "Loss: 4188.894065173061 time: 0.470 iter: 219/500\n",
      "Loss: 4188.894063633379 time: 0.469 iter: 220/500\n",
      "Loss: 4188.894062091564 time: 0.470 iter: 221/500\n",
      "Loss: 4188.894060547685 time: 0.479 iter: 222/500\n",
      "Loss: 4188.894059001739 time: 0.480 iter: 223/500\n",
      "Loss: 4188.894057453649 time: 0.469 iter: 224/500\n",
      "Loss: 4188.894055903296 time: 0.470 iter: 225/500\n",
      "Loss: 4188.8940543505405 time: 0.479 iter: 226/500\n",
      "Loss: 4188.894052795247 time: 0.455 iter: 227/500\n",
      "Loss: 4188.894051237313 time: 0.494 iter: 228/500\n",
      "Loss: 4188.894049676671 time: 0.469 iter: 229/500\n",
      "Loss: 4188.894048113287 time: 0.459 iter: 230/500\n",
      "Loss: 4188.894046547164 time: 0.468 iter: 231/500\n",
      "Loss: 4188.894044978325 time: 0.451 iter: 232/500\n",
      "Loss: 4188.894043406805 time: 0.472 iter: 233/500\n",
      "Loss: 4188.894041832645 time: 0.479 iter: 234/500\n",
      "Loss: 4188.894040255885 time: 0.468 iter: 235/500\n",
      "Loss: 4188.894038676551 time: 0.494 iter: 236/500\n",
      "Loss: 4188.894037094664 time: 0.465 iter: 237/500\n",
      "Loss: 4188.894035510224 time: 0.476 iter: 238/500\n",
      "Loss: 4188.894033923218 time: 0.458 iter: 239/500\n",
      "Loss: 4188.894032333627 time: 0.471 iter: 240/500\n",
      "Loss: 4188.894030741426 time: 0.469 iter: 241/500\n",
      "Loss: 4188.894029146594 time: 0.469 iter: 242/500\n",
      "Loss: 4188.894027549121 time: 0.482 iter: 243/500\n",
      "Loss: 4188.894025949003 time: 0.478 iter: 244/500\n",
      "Loss: 4188.894024346247 time: 0.449 iter: 245/500\n",
      "Loss: 4188.894022740865 time: 0.490 iter: 246/500\n",
      "Loss: 4188.89402113288 time: 0.480 iter: 247/500\n",
      "Loss: 4188.894019522308 time: 0.500 iter: 248/500\n",
      "Loss: 4188.894017909167 time: 0.529 iter: 249/500\n",
      "Loss: 4188.894016293473 time: 0.469 iter: 250/500\n",
      "Loss: 4188.894014675238 time: 0.500 iter: 251/500\n",
      "Loss: 4188.89401305447 time: 0.459 iter: 252/500\n",
      "Loss: 4188.894011431172 time: 0.479 iter: 253/500\n",
      "Loss: 4188.894009805354 time: 0.479 iter: 254/500\n",
      "Loss: 4188.894008177014 time: 0.460 iter: 255/500\n",
      "Loss: 4188.8940065461575 time: 0.489 iter: 256/500\n",
      "Loss: 4188.894004912788 time: 0.473 iter: 257/500\n",
      "Loss: 4188.894003276913 time: 0.466 iter: 258/500\n",
      "Loss: 4188.89400163854 time: 0.460 iter: 259/500\n",
      "Loss: 4188.893999997684 time: 0.469 iter: 260/500\n",
      "Loss: 4188.893998354355 time: 0.450 iter: 261/500\n",
      "Loss: 4188.8939967085635 time: 0.542 iter: 262/500\n",
      "Loss: 4188.8939950603235 time: 0.476 iter: 263/500\n",
      "Loss: 4188.893993409648 time: 0.475 iter: 264/500\n",
      "Loss: 4188.893991756548 time: 0.513 iter: 265/500\n",
      "Loss: 4188.89399010103 time: 0.500 iter: 266/500\n",
      "Loss: 4188.893988443105 time: 0.480 iter: 267/500\n",
      "Loss: 4188.89398678278 time: 0.479 iter: 268/500\n",
      "Loss: 4188.893985120064 time: 0.489 iter: 269/500\n",
      "Loss: 4188.893983454965 time: 0.486 iter: 270/500\n",
      "Loss: 4188.893981787495 time: 0.476 iter: 271/500\n",
      "Loss: 4188.89398011766 time: 0.477 iter: 272/500\n",
      "Loss: 4188.89397844547 time: 0.476 iter: 273/500\n",
      "Loss: 4188.893976770937 time: 0.475 iter: 274/500\n",
      "Loss: 4188.893975094068 time: 0.477 iter: 275/500\n",
      "Loss: 4188.893973414879 time: 0.492 iter: 276/500\n",
      "Loss: 4188.893971733374 time: 0.490 iter: 277/500\n",
      "Loss: 4188.89397004957 time: 0.478 iter: 278/500\n",
      "Loss: 4188.893968363472 time: 0.470 iter: 279/500\n",
      "Loss: 4188.893966675091 time: 0.489 iter: 280/500\n",
      "Loss: 4188.893964984437 time: 0.459 iter: 281/500\n",
      "Loss: 4188.893963291519 time: 0.510 iter: 282/500\n",
      "Loss: 4188.893961596345 time: 0.459 iter: 283/500\n",
      "Loss: 4188.893959898926 time: 0.480 iter: 284/500\n",
      "Loss: 4188.893958199272 time: 0.490 iter: 285/500\n",
      "Loss: 4188.893956497391 time: 0.459 iter: 286/500\n",
      "Loss: 4188.893954793291 time: 0.450 iter: 287/500\n",
      "Loss: 4188.893953086986 time: 0.480 iter: 288/500\n",
      "Loss: 4188.893951378483 time: 0.487 iter: 289/500\n",
      "Loss: 4188.893949667792 time: 0.509 iter: 290/500\n",
      "Loss: 4188.893947954923 time: 0.617 iter: 291/500\n",
      "Loss: 4188.893946239884 time: 0.492 iter: 292/500\n",
      "Loss: 4188.893944522687 time: 0.540 iter: 293/500\n",
      "Loss: 4188.89394280334 time: 0.555 iter: 294/500\n",
      "Loss: 4188.893941081851 time: 0.574 iter: 295/500\n",
      "Loss: 4188.893939358231 time: 0.549 iter: 296/500\n",
      "Loss: 4188.893937632489 time: 0.550 iter: 297/500\n",
      "Loss: 4188.893935904634 time: 0.570 iter: 298/500\n",
      "Loss: 4188.8939341746745 time: 0.551 iter: 299/500\n",
      "Loss: 4188.893932442622 time: 0.523 iter: 300/500\n",
      "Loss: 4188.893930708483 time: 0.544 iter: 301/500\n",
      "Loss: 4188.8939289722675 time: 0.599 iter: 302/500\n",
      "Loss: 4188.893927233986 time: 0.474 iter: 303/500\n",
      "Loss: 4188.893925493646 time: 0.530 iter: 304/500\n",
      "Loss: 4188.893923751257 time: 0.545 iter: 305/500\n",
      "Loss: 4188.893922006829 time: 0.508 iter: 306/500\n",
      "Loss: 4188.8939202603715 time: 0.489 iter: 307/500\n",
      "Loss: 4188.89391851189 time: 0.481 iter: 308/500\n",
      "Loss: 4188.893916761397 time: 0.545 iter: 309/500\n",
      "Loss: 4188.893915008901 time: 0.499 iter: 310/500\n",
      "Loss: 4188.89391325441 time: 0.554 iter: 311/500\n",
      "Loss: 4188.893911497933 time: 0.605 iter: 312/500\n",
      "Loss: 4188.893909739479 time: 0.505 iter: 313/500\n",
      "Loss: 4188.893907979056 time: 0.610 iter: 314/500\n",
      "Loss: 4188.8939062166755 time: 0.520 iter: 315/500\n",
      "Loss: 4188.893904452343 time: 0.539 iter: 316/500\n",
      "Loss: 4188.893902686071 time: 0.525 iter: 317/500\n",
      "Loss: 4188.893900917865 time: 0.539 iter: 318/500\n",
      "Loss: 4188.893899147734 time: 0.483 iter: 319/500\n",
      "Loss: 4188.8938973756885 time: 0.501 iter: 320/500\n",
      "Loss: 4188.8938956017355 time: 0.501 iter: 321/500\n",
      "Loss: 4188.893893825885 time: 0.483 iter: 322/500\n",
      "Loss: 4188.893892048144 time: 0.491 iter: 323/500\n",
      "Loss: 4188.893890268523 time: 0.477 iter: 324/500\n",
      "Loss: 4188.89388848703 time: 0.459 iter: 325/500\n",
      "Loss: 4188.893886703673 time: 0.493 iter: 326/500\n",
      "Loss: 4188.89388491846 time: 0.478 iter: 327/500\n",
      "Loss: 4188.893883131401 time: 0.496 iter: 328/500\n",
      "Loss: 4188.893881342503 time: 0.506 iter: 329/500\n",
      "Loss: 4188.893879551777 time: 0.484 iter: 330/500\n",
      "Loss: 4188.893877759229 time: 0.499 iter: 331/500\n",
      "Loss: 4188.893875964867 time: 0.520 iter: 332/500\n",
      "Loss: 4188.893874168702 time: 0.543 iter: 333/500\n",
      "Loss: 4188.89387237074 time: 0.528 iter: 334/500\n",
      "Loss: 4188.893870570991 time: 0.462 iter: 335/500\n",
      "Loss: 4188.893868769463 time: 0.565 iter: 336/500\n",
      "Loss: 4188.893866966161 time: 0.504 iter: 337/500\n",
      "Loss: 4188.893865161099 time: 0.511 iter: 338/500\n",
      "Loss: 4188.893863354282 time: 0.558 iter: 339/500\n",
      "Loss: 4188.893861545717 time: 0.473 iter: 340/500\n",
      "Loss: 4188.893859735415 time: 0.496 iter: 341/500\n",
      "Loss: 4188.893857923383 time: 0.499 iter: 342/500\n",
      "Loss: 4188.893856109628 time: 0.500 iter: 343/500\n",
      "Loss: 4188.8938542941605 time: 0.509 iter: 344/500\n",
      "Loss: 4188.893852476987 time: 0.480 iter: 345/500\n",
      "Loss: 4188.893850658116 time: 0.490 iter: 346/500\n",
      "Loss: 4188.893848837556 time: 0.509 iter: 347/500\n",
      "Loss: 4188.893847015316 time: 0.491 iter: 348/500\n",
      "Loss: 4188.8938451914 time: 0.521 iter: 349/500\n",
      "Loss: 4188.89384336582 time: 0.507 iter: 350/500\n",
      "Loss: 4188.893841538584 time: 0.495 iter: 351/500\n",
      "Loss: 4188.893839709696 time: 0.464 iter: 352/500\n",
      "Loss: 4188.893837879167 time: 0.480 iter: 353/500\n",
      "Loss: 4188.893836047007 time: 0.490 iter: 354/500\n",
      "Loss: 4188.89383421322 time: 0.470 iter: 355/500\n",
      "Loss: 4188.893832377815 time: 0.460 iter: 356/500\n",
      "Loss: 4188.893830540799 time: 0.480 iter: 357/500\n",
      "Loss: 4188.893828702184 time: 0.483 iter: 358/500\n",
      "Loss: 4188.893826861971 time: 0.466 iter: 359/500\n",
      "Loss: 4188.893825020176 time: 0.500 iter: 360/500\n",
      "Loss: 4188.8938231768 time: 0.479 iter: 361/500\n",
      "Loss: 4188.893821331854 time: 0.479 iter: 362/500\n",
      "Loss: 4188.8938194853445 time: 0.473 iter: 363/500\n",
      "Loss: 4188.893817637281 time: 0.466 iter: 364/500\n",
      "Loss: 4188.893815787667 time: 0.502 iter: 365/500\n",
      "Loss: 4188.8938139365155 time: 0.466 iter: 366/500\n",
      "Loss: 4188.89381208383 time: 0.472 iter: 367/500\n",
      "Loss: 4188.8938102296215 time: 0.476 iter: 368/500\n",
      "Loss: 4188.893808373894 time: 0.476 iter: 369/500\n",
      "Loss: 4188.89380651666 time: 0.474 iter: 370/500\n",
      "Loss: 4188.893804657922 time: 0.490 iter: 371/500\n",
      "Loss: 4188.893802797691 time: 0.481 iter: 372/500\n",
      "Loss: 4188.893800935972 time: 0.466 iter: 373/500\n",
      "Loss: 4188.893799072774 time: 0.489 iter: 374/500\n",
      "Loss: 4188.893797208105 time: 0.469 iter: 375/500\n",
      "Loss: 4188.893795341972 time: 0.473 iter: 376/500\n",
      "Loss: 4188.893793474381 time: 0.466 iter: 377/500\n",
      "Loss: 4188.893791605341 time: 0.480 iter: 378/500\n",
      "Loss: 4188.89378973486 time: 0.459 iter: 379/500\n",
      "Loss: 4188.893787862944 time: 0.473 iter: 380/500\n",
      "Loss: 4188.893785989601 time: 0.466 iter: 381/500\n",
      "Loss: 4188.893784114839 time: 0.479 iter: 382/500\n",
      "Loss: 4188.893782238665 time: 0.485 iter: 383/500\n",
      "Loss: 4188.893780361084 time: 0.490 iter: 384/500\n",
      "Loss: 4188.893778482107 time: 0.484 iter: 385/500\n",
      "Loss: 4188.893776601739 time: 0.502 iter: 386/500\n",
      "Loss: 4188.893774719989 time: 0.488 iter: 387/500\n",
      "Loss: 4188.893772836863 time: 0.458 iter: 388/500\n",
      "Loss: 4188.893770952367 time: 0.460 iter: 389/500\n",
      "Loss: 4188.893769066512 time: 0.463 iter: 390/500\n",
      "Loss: 4188.893767179299 time: 0.479 iter: 391/500\n",
      "Loss: 4188.8937652907425 time: 0.480 iter: 392/500\n",
      "Loss: 4188.893763400845 time: 0.490 iter: 393/500\n",
      "Loss: 4188.893761509616 time: 0.479 iter: 394/500\n",
      "Loss: 4188.893759617061 time: 0.479 iter: 395/500\n",
      "Loss: 4188.893757723187 time: 0.466 iter: 396/500\n",
      "Loss: 4188.893755828002 time: 0.463 iter: 397/500\n",
      "Loss: 4188.893753931514 time: 0.460 iter: 398/500\n",
      "Loss: 4188.893752033728 time: 0.469 iter: 399/500\n",
      "Loss: 4188.893750134654 time: 0.461 iter: 400/500\n",
      "Loss: 4188.893748234292 time: 0.468 iter: 401/500\n",
      "Loss: 4188.893746332658 time: 0.514 iter: 402/500\n",
      "Loss: 4188.893744429755 time: 0.465 iter: 403/500\n",
      "Loss: 4188.893742525589 time: 0.491 iter: 404/500\n",
      "Loss: 4188.8937406201685 time: 0.468 iter: 405/500\n",
      "Loss: 4188.893738713499 time: 0.479 iter: 406/500\n",
      "Loss: 4188.8937368055895 time: 0.462 iter: 407/500\n",
      "Loss: 4188.893734896446 time: 0.467 iter: 408/500\n",
      "Loss: 4188.8937329860755 time: 0.475 iter: 409/500\n",
      "Loss: 4188.893731074483 time: 0.485 iter: 410/500\n",
      "Loss: 4188.893729161679 time: 0.474 iter: 411/500\n",
      "Loss: 4188.893727247667 time: 0.459 iter: 412/500\n",
      "Loss: 4188.893725332456 time: 0.474 iter: 413/500\n",
      "Loss: 4188.89372341605 time: 0.534 iter: 414/500\n",
      "Loss: 4188.893721498459 time: 0.455 iter: 415/500\n",
      "Loss: 4188.89371957969 time: 0.470 iter: 416/500\n",
      "Loss: 4188.893717659747 time: 0.550 iter: 417/500\n",
      "Loss: 4188.893715738638 time: 0.480 iter: 418/500\n",
      "Loss: 4188.89371381637 time: 0.469 iter: 419/500\n",
      "Loss: 4188.89371189295 time: 0.510 iter: 420/500\n",
      "Loss: 4188.8937099683835 time: 0.541 iter: 421/500\n",
      "Loss: 4188.893708042679 time: 0.551 iter: 422/500\n",
      "Loss: 4188.89370611584 time: 0.449 iter: 423/500\n",
      "Loss: 4188.893704187877 time: 0.503 iter: 424/500\n",
      "Loss: 4188.893702258795 time: 0.517 iter: 425/500\n",
      "Loss: 4188.8937003286 time: 0.569 iter: 426/500\n",
      "Loss: 4188.8936983973 time: 0.590 iter: 427/500\n",
      "Loss: 4188.8936964649 time: 0.549 iter: 428/500\n",
      "Loss: 4188.893694531408 time: 0.539 iter: 429/500\n",
      "Loss: 4188.893692596828 time: 0.559 iter: 430/500\n",
      "Loss: 4188.893690661171 time: 0.581 iter: 431/500\n",
      "Loss: 4188.89368872444 time: 0.568 iter: 432/500\n",
      "Loss: 4188.893686786641 time: 0.580 iter: 433/500\n",
      "Loss: 4188.893684847784 time: 0.570 iter: 434/500\n",
      "Loss: 4188.893682907872 time: 0.589 iter: 435/500\n",
      "Loss: 4188.893680966915 time: 0.580 iter: 436/500\n",
      "Loss: 4188.893679024915 time: 0.562 iter: 437/500\n",
      "Loss: 4188.893677081882 time: 0.572 iter: 438/500\n",
      "Loss: 4188.893675137821 time: 0.534 iter: 439/500\n",
      "Loss: 4188.893673192739 time: 0.540 iter: 440/500\n",
      "Loss: 4188.893671246642 time: 0.532 iter: 441/500\n",
      "Loss: 4188.893669299536 time: 0.540 iter: 442/500\n",
      "Loss: 4188.893667351429 time: 0.557 iter: 443/500\n",
      "Loss: 4188.8936654023255 time: 0.569 iter: 444/500\n",
      "Loss: 4188.893663452233 time: 0.539 iter: 445/500\n",
      "Loss: 4188.893661501155 time: 0.585 iter: 446/500\n",
      "Loss: 4188.893659549102 time: 0.484 iter: 447/500\n",
      "Loss: 4188.893657596078 time: 0.476 iter: 448/500\n",
      "Loss: 4188.89365564209 time: 0.463 iter: 449/500\n",
      "Loss: 4188.893653687144 time: 0.520 iter: 450/500\n",
      "Loss: 4188.893651731246 time: 0.471 iter: 451/500\n",
      "Loss: 4188.8936497744035 time: 0.510 iter: 452/500\n",
      "Loss: 4188.893647816621 time: 0.490 iter: 453/500\n",
      "Loss: 4188.893645857904 time: 0.498 iter: 454/500\n",
      "Loss: 4188.893643898261 time: 0.523 iter: 455/500\n",
      "Loss: 4188.893641937697 time: 0.522 iter: 456/500\n",
      "Loss: 4188.893639976219 time: 0.574 iter: 457/500\n",
      "Loss: 4188.893638013832 time: 0.523 iter: 458/500\n",
      "Loss: 4188.893636050543 time: 0.536 iter: 459/500\n",
      "Loss: 4188.893634086357 time: 0.520 iter: 460/500\n",
      "Loss: 4188.893632121282 time: 0.545 iter: 461/500\n",
      "Loss: 4188.893630155323 time: 0.543 iter: 462/500\n",
      "Loss: 4188.893628188485 time: 0.553 iter: 463/500\n",
      "Loss: 4188.893626220775 time: 0.558 iter: 464/500\n",
      "Loss: 4188.8936242522 time: 0.554 iter: 465/500\n",
      "Loss: 4188.893622282765 time: 0.590 iter: 466/500\n",
      "Loss: 4188.893620312476 time: 0.501 iter: 467/500\n",
      "Loss: 4188.89361834134 time: 0.478 iter: 468/500\n",
      "Loss: 4188.8936163693625 time: 0.499 iter: 469/500\n",
      "Loss: 4188.893614396547 time: 0.500 iter: 470/500\n",
      "Loss: 4188.893612422904 time: 0.499 iter: 471/500\n",
      "Loss: 4188.893610448434 time: 0.512 iter: 472/500\n",
      "Loss: 4188.893608473148 time: 0.500 iter: 473/500\n",
      "Loss: 4188.89360649705 time: 0.518 iter: 474/500\n",
      "Loss: 4188.893604520145 time: 0.500 iter: 475/500\n",
      "Loss: 4188.893602542441 time: 0.499 iter: 476/500\n",
      "Loss: 4188.893600563942 time: 0.502 iter: 477/500\n",
      "Loss: 4188.893598584654 time: 0.508 iter: 478/500\n",
      "Loss: 4188.893596604583 time: 0.539 iter: 479/500\n",
      "Loss: 4188.893594623735 time: 0.476 iter: 480/500\n",
      "Loss: 4188.893592642118 time: 0.493 iter: 481/500\n",
      "Loss: 4188.8935906597335 time: 0.474 iter: 482/500\n",
      "Loss: 4188.893588676593 time: 0.499 iter: 483/500\n",
      "Loss: 4188.893586692695 time: 0.490 iter: 484/500\n",
      "Loss: 4188.89358470805 time: 0.479 iter: 485/500\n",
      "Loss: 4188.893582722665 time: 0.496 iter: 486/500\n",
      "Loss: 4188.8935807365415 time: 0.503 iter: 487/500\n",
      "Loss: 4188.8935787496885 time: 0.520 iter: 488/500\n",
      "Loss: 4188.893576762111 time: 0.529 iter: 489/500\n",
      "Loss: 4188.893574773814 time: 0.530 iter: 490/500\n",
      "Loss: 4188.893572784802 time: 0.516 iter: 491/500\n",
      "Loss: 4188.893570795083 time: 0.540 iter: 492/500\n",
      "Loss: 4188.893568804663 time: 0.509 iter: 493/500\n",
      "Loss: 4188.893566813545 time: 0.541 iter: 494/500\n",
      "Loss: 4188.8935648217375 time: 0.543 iter: 495/500\n",
      "Loss: 4188.893562829244 time: 0.515 iter: 496/500\n",
      "Loss: 4188.893560836072 time: 0.509 iter: 497/500\n",
      "Loss: 4188.893558842223 time: 0.501 iter: 498/500\n",
      "Loss: 4188.893556847707 time: 0.540 iter: 499/500\n",
      "Training time: 249.5612\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "~~ L-BFGS optimization ~~\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer dense_1 weight shape (30, 30) is not compatible with provided weight shape (2, 30).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 163\u001b[0m\n\u001b[0;32m    157\u001b[0m checkpoint_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Checkpoint/1D_flame_theta_as_data_loss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# model.model.load_weights('./Checkpoint/1D_theta_mp_1000_50000_nondim_30_6_init_10_10_100.index')\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# model.model.load_weights(checkpoint_str_NN)\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43madam_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbfgs_max_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m#%%\u001b[39;00m\n\u001b[0;32m    168\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_str)\n",
      "File \u001b[1;32m~\\Desktop\\Uni2\\Proejkt\\Robin\\Code\\Burger_setup_vikas\\src\\../optimizer\\NeuralNet_optimizer.py:198\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[1;34m(self, Adam_iterations, LBFGS_max_iterations, checkpoint_str, lr_epochs, lr_list)\u001b[0m\n\u001b[0;32m    195\u001b[0m maxiter \u001b[38;5;241m=\u001b[39m LBFGS_max_iterations\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_last_callback \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 198\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_flat_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxfun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m500000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxcor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxls\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mftol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m optimal_w \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mx \n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_weights(optimal_w)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[1;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[0;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 71\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\Uni2\\Proejkt\\Robin\\Code\\Burger_setup_vikas\\src\\../optimizer\\NeuralNet_optimizer.py:97\u001b[0m, in \u001b[0;36mPhysicsInformedNN.loss_and_flat_grad\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_and_flat_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, w):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 97\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m         loss_value, _, _, _, _, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss()\n\u001b[0;32m     99\u001b[0m     grad \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32m~\\Desktop\\Uni2\\Proejkt\\Robin\\Code\\Burger_setup_vikas\\src\\../optimizer\\NeuralNet_optimizer.py:85\u001b[0m, in \u001b[0;36mSeq_NN.set_weights\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m     83\u001b[0m biases \u001b[38;5;241m=\u001b[39m w_b[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msizes_w[i]:]\n\u001b[0;32m     84\u001b[0m weights_biases \u001b[38;5;241m=\u001b[39m [weights, biases]\n\u001b[1;32m---> 85\u001b[0m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_biases\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:1832\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1830\u001b[0m ref_shape \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ref_shape\u001b[38;5;241m.\u001b[39mis_compatible_with(weight_shape):\n\u001b[1;32m-> 1832\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1833\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weight shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1834\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not compatible with provided weight \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1835\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1836\u001b[0m     )\n\u001b[0;32m   1837\u001b[0m weight_value_tuples\u001b[38;5;241m.\u001b[39mappend((param, weight))\n\u001b[0;32m   1838\u001b[0m weight_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer dense_1 weight shape (30, 30) is not compatible with provided weight shape (2, 30)."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Feb 11 08:54:08 2023\n",
    "\n",
    "@author: vikas\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from scipy import io as sio\n",
    "# from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from scipy.stats import qmc\n",
    "from scipy.interpolate import griddata\n",
    "import h5py\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy import interpolate\n",
    "from PDE import PINN_PDE\n",
    "from Data_NN import Data_NN\n",
    "from pyDOE import lhs\n",
    "\n",
    "#%% Constants\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#umin = np.array([umin]) \n",
    "#umax = np.array([umax]) \n",
    "\n",
    "# Read the data from reference file\n",
    "hf = h5py.File('./Data_Burgers/burgers_shock.h5','r')\n",
    "t = np.array(hf.get('/t')).T\n",
    "x = np.array(hf.get('/x')).T\n",
    "Exact = np.array(hf.get('/usol'))\n",
    "hf.close()\n",
    "nu = 0.01/np.pi\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_all = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "Uexact_all = Exact.flatten()[:,None]              \n",
    "\n",
    "# Domain bounds\n",
    "lb = X_all.min(0)\n",
    "ub = X_all.max(0)\n",
    "\n",
    "scale_out = [2.0, 1.0]\n",
    "umin = Exact.min()\n",
    "umax = Exact.max()\n",
    "scale= [umin,umax]\n",
    "\n",
    "# First, we define the points associated with the initial condition, which is known\n",
    "# Uniformly distributed points in x-domain [-1,1] for the initial condition\n",
    "# u = -sin(pi*x)\n",
    "\n",
    "Nx_init = 50\n",
    "x_init = np.linspace(lb[0],ub[0],Nx_init)\n",
    "x_init_scaled = x_init/ub[0]\n",
    "#print(x_init)\n",
    "u_x_init = -1*np.sin(np.pi*x_init)\n",
    "u_x_init_scaled = u_x_init/umax\n",
    "#print(u_x_init)\n",
    "\n",
    "X_init = np.hstack((x_init.reshape(-1,1), np.zeros(len(x_init)).reshape(-1,1)))\n",
    "X_init_scaled = np.hstack((x_init_scaled.reshape(-1,1), np.zeros(len(x_init)).reshape(-1,1)))\n",
    "\n",
    "\n",
    "\n",
    "# Second, we define the points associated with the left and right Dirichlet boundary conditions (u=0)\n",
    "# Uniformly distributed points in t-domain [0,1] for the left BC\n",
    "\n",
    "Nt_BC = 50\n",
    "\n",
    "t_BC = np.linspace(lb[1], ub[1], Nt_BC)\n",
    "t_BC_scaled = t_BC/ub[1]\n",
    "u_train = np.zeros(len(t_BC))\n",
    "u_train_scaled = u_train/umax\n",
    "\n",
    "x_scaled_init = -np.ones(len(t_BC))\n",
    "x_scaled_init = x_scaled_init/ub[0]\n",
    "\n",
    "T_init = np.hstack((-x_scaled_init.reshape(-1,1),t_BC_scaled.reshape(-1,1) ))\n",
    "#print(T_init)\n",
    "\n",
    "# Uniformly distributed points in t-domain [0,1] for the right BC\n",
    "\n",
    "T_init1 = np.hstack((x_scaled_init.reshape(-1,1),t_BC_scaled.reshape(-1,1) ))\n",
    "#print(T_init1)\n",
    "\n",
    "\n",
    "# Stacking all the features\n",
    "\n",
    "X_train = np.vstack((X_init_scaled, T_init, T_init1))\n",
    "u_train = np.vstack((u_x_init_scaled.reshape(-1,1), u_train_scaled.reshape(-1,1), u_train_scaled.reshape(-1,1)))\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def zufallswerte(a, b, anzahl=3):\n",
    "    \n",
    "    zufallsindices = random.sample(range(len(a)), anzahl)\n",
    "    zufallswerte_a = [a[i] for i in zufallsindices]\n",
    "    zugehoerige_werte_b = [b[i] for i in zufallsindices]\n",
    "    \n",
    "    return zufallswerte_a, zugehoerige_werte_b\n",
    "\n",
    "\n",
    "x_t_sim,u_sim = zufallswerte(X_all, Uexact_all, 500)\n",
    "x_t_sim_scaled = x_t_sim/ub\n",
    "u_sim_scaled = u_sim/umax\n",
    "\n",
    "N_colloc = 5000\n",
    "\n",
    "X_colloc_train = lb + (ub-lb)*lhs(2,N_colloc)\n",
    "X_colloc_train_scaled = X_colloc_train/ub\n",
    "\n",
    "\n",
    "layers = [2, 30, 30, 30, 30, 30, 30, 1]  \n",
    "\n",
    "scale_max = tf.convert_to_tensor(np.array([umax, 2.]))\n",
    "scale_min = tf.convert_to_tensor(np.array([umin, 0.]))\n",
    "x_t_boundary = tf.convert_to_tensor(X_train)\n",
    "u_boundary = tf.convert_to_tensor(u_train)\n",
    "x_t_colloc = tf.convert_to_tensor(X_colloc_train_scaled)\n",
    "x_t_sim = tf.convert_to_tensor(x_t_sim_scaled)\n",
    "u_sim = tf.convert_to_tensor(u_sim_scaled)\n",
    "\n",
    "model = PINN_PDE(x_t_boundary,\n",
    "                 u_boundary,\n",
    "                 x_t_colloc,\n",
    "                 x_t_sim,\n",
    "                 u_sim,\n",
    "                 ub, lb, \n",
    "                 layers,\n",
    "                 [scale_max, scale_min],\n",
    "                 scale_out,\n",
    "                 nu\n",
    "                 )\n",
    "\n",
    "#%%\n",
    "\n",
    "adam_iterations = 500  # Number of training steps \n",
    "lbfgs_max_iterations = 10000 # Max iterations for lbfgs\n",
    "\n",
    "lr_list = [1e-3,1e-4]\n",
    "lr_epochs = [200]\n",
    "\n",
    "# Loading the weights from the initialized fields for faster convergence\n",
    "# model.model.load_weights(checkpoint_str_NN)\n",
    "\n",
    "#%%\n",
    "\n",
    "#### Training\n",
    "checkpoint_str = './Checkpoint/1D_flame_theta_as_data_loss'\n",
    "   \n",
    "# model.model.load_weights('./Checkpoint/1D_theta_mp_1000_50000_nondim_30_6_init_10_10_100.index')\n",
    "# model.model.load_weights(checkpoint_str_NN)\n",
    "\n",
    "\n",
    "hist = model.train(adam_iterations, lbfgs_max_iterations, checkpoint_str, lr_epochs, lr_list)\n",
    "# \n",
    "\n",
    "#%%\n",
    "\n",
    "model.model.load_weights(checkpoint_str)\n",
    "# \n",
    "#%% Field prediction\n",
    "\n",
    "u_pred = model.predict(X_all)\n",
    "\n",
    "u_pred = np.array(u_pred)*U_IN\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(Input_all[:,0], u_pred)\n",
    "# plt.plot(Input_all[:,0], u)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#%%  plotting functions\n",
    "\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=28)\n",
    "plt.rc('xtick', labelsize=28)\n",
    "plt.rc('ytick', labelsize=28)\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize=(15,6))\n",
    "\n",
    "\n",
    "ax1 = ax.twinx()\n",
    "ax.plot(z,rho, label='Cantera density', linewidth=3, color='#cc0000')\n",
    "# ax.plot(X_colloc[0:10],rho_pred[0:10], 'o', linewidth=3, color='#cc0000', markersize=10, fillstyle='none')\n",
    "ax.plot(X_colloc[10:-10:10],rho_pred[10:-10:10], 'o', label='PINN density', \n",
    "        linewidth=3, color='#cc0000', markersize=10, fillstyle='none')\n",
    "# ax.plot(z[-10:],rho_pred[-10:], 'o', linewidth=3, color='#cc0000', markersize=10, fillstyle='none')\n",
    "\n",
    "ax.set_xlabel('Length')\n",
    "ax.set_ylabel(r'Density $(kg/m)$')\n",
    "ax.legend(ncol=1,handleheight=1.4, labelspacing=0.0, handletextpad=0.2,\n",
    "                borderpad=0.2, loc='best')\n",
    "ax.set_xticks([0, 0.004, 0.008, 0.012, 0.016, 0.02])\n",
    "\n",
    "ax1.plot(z,u, label='Cantera velocity', linewidth=3, color='#0000CD')\n",
    "# ax1.plot(z[0:10],u_pred[0:10], 'd', linewidth=3, color='#228B22', markersize=10, fillstyle='none')\n",
    "ax1.plot(X_colloc[10:-10:10],u_pred[10:-10:10], 'd', label='PINN velocity', linewidth=3, color='#0000CD',\n",
    "          markersize=10, fillstyle='none')\n",
    "# ax1.plot(z[-10:],u_pred[-10:], 'd', linewidth=3, color='#228B22', markersize=10, fillstyle='none')\n",
    "\n",
    "ax1.set_ylabel(r'Velocity $(m/s)$')\n",
    "# ax1.legend(ncol=1,handleheight=1.4, labelspacing=0.0, handletextpad=0.2,\n",
    "#                 borderpad=0.2, loc='center right')\n",
    "\n",
    "# ax1.plot(z,T, label='Cantera temperature', linewidth=3, color='#228B22')\n",
    "# # ax1.plot(z[0:10],T_pred[0:10], 'd', linewidth=3, color='#228B22', markersize=10, fillstyle='none')\n",
    "# ax1.plot(X_colloc[10:-10:10],T_pred[10:-10:10], 'd', label='PINN temperature', linewidth=3, color='#228B22',\n",
    "#           markersize=10, fillstyle='none')\n",
    "# # ax1.plot(z[-10:],T_pred[-10:], 'd', linewidth=3, color='#228B22', markersize=10, fillstyle='none')\n",
    "\n",
    "# ax1.set_ylabel(r'Temperature $(K)$')\n",
    "ax1.legend(ncol=1,handleheight=1.4, labelspacing=0.0, handletextpad=0.2,\n",
    "                borderpad=0.2, loc='best')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "######  Loss curve\n",
    "\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "\n",
    "##### Plotting losses\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "\n",
    "\n",
    "ax.semilogy(model.Adam_hist + model.LBFGS_hist, linewidth=3, label='Total loss',\n",
    "            color='#1f77b4')\n",
    "ax.semilogy(model.mass_cons, linewidth=3, label='Mass cons loss', \n",
    "            color='#ff7f0e')\n",
    "\n",
    "ax.semilogy(model.bound_loss, linewidth=3, label='Dirichlet loss',\n",
    "            color='#9467bd')\n",
    "ax.semilogy(model.progress_cons, linewidth=3, label='Progress loss',\n",
    "            color='#8c564b') \n",
    "ax.semilogy(model.data_loss, linewidth=3, label='Data loss',\n",
    "            color='#d62728')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "ax.set_ylim([1e-6, 1e9])\n",
    "\n",
    "ax.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973a5a5-7577-44cf-b3a6-c7a9b4a53a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
